{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Task1_Word_Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonisali/ADFB_SpringerNature/blob/master/Task1_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VZXi_KGi0UR"
      },
      "source": [
        "# Task 1: Word Embeddings (10 points)\r\n",
        "\r\n",
        "This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48t-II1vkuau"
      },
      "source": [
        "## Imports\r\n",
        "\r\n",
        "This code block is reserved for your imports. \r\n",
        "\r\n",
        "You are free to use the following packages: \r\n",
        "\r\n",
        "(List of packages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kh6nh84-AOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09867636-0b86-4600-df23-b15ff7d99a85"
      },
      "source": [
        "# Imports\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import itertools as it\r\n",
        "import spacy\r\n",
        "from spacy.lang.hi import Hindi\r\n",
        "nlp_hi = Hindi()\r\n",
        "import regex as re\r\n",
        "!pip install demoji\r\n",
        "import demoji\r\n",
        "demoji.download_codes()\r\n",
        "!pip install emot\r\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\r\n",
        "!pip install emoji"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133kB 6.6MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWmk3hVllEcU"
      },
      "source": [
        "# 1.1 Get the data (0.5 points)\r\n",
        "\r\n",
        "The Hindi portion HASOC corpus from [github.io](https://hasocfire.github.io/hasoc/2019/dataset.html) is already available in the repo, at data/hindi_hatespeech.tsv . Load it into a data structure of your choice. Then, split off a small part of the corpus as a development set (~100 data points).\r\n",
        "\r\n",
        "If you are using Colab the first two lines will let you upload folders or files from your local file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtI7DJ-0-AOP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fa0eee75-c61c-4462-8328-50fe62f94938"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "\n",
        "data_set = pd.read_csv(\"hindi_hatespeech.tsv\",sep='\\t')\n",
        "\n",
        "#Making dev set:   \n",
        "##we use sample function to randomly pick 100 rows as specified in question.\n",
        "dev_set = data_set.sample(n=100)\n",
        "dev_set.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>task_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2859</th>\n",
              "      <td>hasoc_hi_1498</td>\n",
              "      <td>‡§∏‡•Å‡§Ö‡§∞ ‡§¨‡•Å‡§ñ‡§æ‡§∞‡•Ä ‡§Ö‡§¨ ‡§§‡•á‡§∞‡•á ‡§ï‡•å‡§Æ ‡§ï‡§æ ‡§Ö‡§Ç‡§§ ‡§®‡§ø‡§ï‡§ü ‡§Ü‡§§‡•á ‡§ú‡§æ ‡§∞‡§π‡§æ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>HATE</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3853</th>\n",
              "      <td>hasoc_hi_2940</td>\n",
              "      <td>‡§Ö‡§¨ ‡§Ø‡§π ‡§¨‡§§‡§æ‡§®‡§æ ‡§≠‡•Ä ‡§ó‡•Å‡§®‡§æ‡§π ‡§π‡•à ‡§ï‡•Ä #‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§≠‡•Ä ‡§Ø‡•ã‡§ó‡§æ ‡§ï‡§∞ ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3324</th>\n",
              "      <td>hasoc_hi_7576</td>\n",
              "      <td>‡§®‡•á‡§§‡§æ ‡§ö‡§æ‡§≤‡§æ‡§ï ‡§π‡•à ‡§µ‡•ã ‡§ï‡•Å‡§∞‡•ç‡§∏‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ß‡§∞‡•ç‡§Æ ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§™‡§∞ ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>hasoc_hi_3051</td>\n",
              "      <td>‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ö‡§∏‡§Ç‡§≠‡§µ ‡§ï‡•ã ‡§∏‡§Ç‡§≠‡§µ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•Ä ‡§Æ‡§π‡§æ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3504</th>\n",
              "      <td>hasoc_hi_4231</td>\n",
              "      <td>‡§∂‡§∞‡•ç‡§Æ‡§ø‡§Ç‡§¶‡§ó‡•Ä ‡§®‡§ø‡§∞‡•Å‡§§‡•ç‡§§‡§∞ ‡§ï‡§∞ ‡§π‡•Ä ‡§¶‡•á‡§§‡•Ä ‡§π‡•à :    ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§á...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            text_id  ... task_3\n",
              "2859  hasoc_hi_1498  ...    TIN\n",
              "3853  hasoc_hi_2940  ...    TIN\n",
              "3324  hasoc_hi_7576  ...   NONE\n",
              "265   hasoc_hi_3051  ...   NONE\n",
              "3504  hasoc_hi_4231  ...   NONE\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-mSJ8nUlupB"
      },
      "source": [
        "## 1.2 Data preparation (0.5 + 0.5 points)\r\n",
        "\r\n",
        "* Prepare the data by removing everything that does not contain information. \r\n",
        "User names (starting with '@') and punctuation symbols clearly do not convey information, but we also want to get rid of so-called [stopwords](https://en.wikipedia.org/wiki/Stop_word), i. e. words that have little to no semantic content (and, but, yes, the...). Hindi stopwords can be found [here](https://github.com/stopwords-iso/stopwords-hi/blob/master/stopwords-hi.txt) Then, standardize the spelling by lowercasing all words.\r\n",
        "Do this for the development section of the corpus for now.\r\n",
        "\r\n",
        "* What about hashtags (starting with '#') and emojis? Should they be removed too? Justify your answer in the report, and explain how you accounted for this in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkD353LjnOCF"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import itertools as it\r\n",
        "import spacy\r\n",
        "from spacy.lang.hi import Hindi\r\n",
        "import regex as re\r\n",
        "import emoji\r\n",
        "#from spacymoji import Emoji\r\n",
        "nlp_hi = Hindi()\r\n",
        "demoji.download_codes()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "stop_words = open(\"stop_word_hindi.txt\",\"r\")\r\n",
        "stopword_list = stop_words.readlines()\r\n",
        "stop_words.close()\r\n",
        "converted_list = []\r\n",
        "for element in stopword_list:\r\n",
        "    converted_list.append(element.strip())\r\n",
        "print(converted_list)\r\n",
        "\r\n",
        "for stopword in stopword_list:\r\n",
        "    lexeme = nlp_hi.vocab[stopword]\r\n",
        "    lexeme.is_stop = True\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcLGahnQr2wV"
      },
      "source": [
        "def preprocessing_hi(tweet):\r\n",
        "  tweet_hi = []\r\n",
        "  \r\n",
        "  tweet = re.sub('http\\S+\\s*', '', tweet)  # remove URLs\r\n",
        "  tweet = re.sub('RT|cc', '', tweet)  # remove RT and cc\r\n",
        "  tweet = re.sub('#\\S+', '', tweet)  # remove hashtags\r\n",
        "  tweet = re.sub('@\\S+', '', tweet)  # remove mentions\r\n",
        "  tweet = demoji.replace(tweet,\" \") #remove smoji's\r\n",
        "  tweet = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', tweet)  # remove punctuations\r\n",
        "  tweet = re.sub('\\s+', ' ', tweet)  # remove extra whitespace\r\n",
        "    \r\n",
        "    \r\n",
        "  # ******* the following piece shall check for hindi specific text-preprocessing using spacy.hindi. lan. library ***** \r\n",
        "  # ******* as well as catches exception from previous block **********************************************************  \r\n",
        "  \r\n",
        "  tokenized_text = nlp_hi(tweet)\r\n",
        "  for token in tokenized_text:\r\n",
        "    if(token.text!='\\n\\n' \r\n",
        "      and (re.search('#\\S+',token.text)is None)\r\n",
        "      and (re.search(r'@\\S+',token.text) is None)\r\n",
        "      and not token.is_stop \r\n",
        "      and not token.is_punct \r\n",
        "      and not token.is_space \r\n",
        "      and not token.like_email\r\n",
        "      and not token.is_digit\r\n",
        "      and not token.is_quote\r\n",
        "      and not token.like_url):\r\n",
        "     tweet_hi.append(token.lemma_)\r\n",
        "  \r\n",
        "  \r\n",
        "  tweet = ' '.join([token  for token in tweet_hi])\r\n",
        "   \r\n",
        "  return tweet"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyrTFS8G2AB5",
        "outputId": "bd8d4e84-db59-4b70-d9df-60d785506050"
      },
      "source": [
        "#from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "tes = \" #gol #gol So 9 ‡§Ü‡§ú ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ï‡•á ‡§§‡§æ‡§≤‡§ï‡§ü‡•ã‡§∞‡§æ ‡§∏‡•ç‡§ü‡•á‡§°‡§ø‡§Ø‡§Æ ‡§Æ‡•á‡§Ç ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó,‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§∏‡•å‡§ú‡§®‡•ç‡§Ø ‡§∏‡•á ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ '‡§π‡•à‡§™‡•Ä‡§®‡•á‡§∏ ‡§è‡§ú‡•Å‡§ï‡•á‡§∂‡§®' ‡§ï‡•á ‡§∏‡§Æ‡§æ‡§™‡§® ‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡•ç‡§Æ‡§ø‡§≤‡§ø‡§§ ‡§π‡•Å‡§Ü üòÄ‡•§ ‡§á‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï,‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§ï‡•á ‡§â‡§ö‡•ç‡§ö‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä ‡§µ ‡§∏‡•ç‡§ï‡•Ç‡§≤‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§®‡•á ‡§Ö‡§™‡§®‡•Ä ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡§æ‡§à‡•§ #Budget2019  @AAPDelhi @AamAadmiParty  https://t.co/zhNmdReb3A\"\r\n",
        "print(tes)\r\n",
        "print(\"Cleaned Tweet is:\",clean_tweet(tes))\r\n",
        "now = clean_tweet(tes) \r\n",
        "tweet = print(\"Pre-processed tweet is:\", preprocessing_hi(now))\r\n",
        "\r\n",
        "#**** All the tweets from dev.set are pre-processed ****\r\n",
        "cleaned_tweets = []\r\n",
        "for t in dev_set.text:\r\n",
        "  cleaned_tweets.append(preprocessing_hi(t))\r\n",
        "\r\n",
        "df = pd.DataFrame(cleaned_tweets, columns = ['cleaned_tweets'])\r\n",
        "print(cleaned_tweets[:-5])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " #gol #gol So 9 ‡§Ü‡§ú ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ï‡•á ‡§§‡§æ‡§≤‡§ï‡§ü‡•ã‡§∞‡§æ ‡§∏‡•ç‡§ü‡•á‡§°‡§ø‡§Ø‡§Æ ‡§Æ‡•á‡§Ç ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó,‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§∏‡•å‡§ú‡§®‡•ç‡§Ø ‡§∏‡•á ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ '‡§π‡•à‡§™‡•Ä‡§®‡•á‡§∏ ‡§è‡§ú‡•Å‡§ï‡•á‡§∂‡§®' ‡§ï‡•á ‡§∏‡§Æ‡§æ‡§™‡§® ‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡•ç‡§Æ‡§ø‡§≤‡§ø‡§§ ‡§π‡•Å‡§Ü üòÄ‡•§ ‡§á‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï,‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§ï‡•á ‡§â‡§ö‡•ç‡§ö‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä ‡§µ ‡§∏‡•ç‡§ï‡•Ç‡§≤‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§®‡•á ‡§Ö‡§™‡§®‡•Ä ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡§æ‡§à‡•§ #Budget2019  @AAPDelhi @AamAadmiParty  https://t.co/zhNmdReb3A\n",
            "Cleaned Tweet is:  So 9 ‡§Ü‡§ú ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ï‡•á ‡§§‡§æ‡§≤‡§ï‡§ü‡•ã‡§∞‡§æ ‡§∏‡•ç‡§ü‡•á‡§°‡§ø‡§Ø‡§Æ ‡§Æ‡•á‡§Ç ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§∏‡•å‡§ú‡§®‡•ç‡§Ø ‡§∏‡•á ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§π‡•à‡§™‡•Ä‡§®‡•á‡§∏ ‡§è‡§ú‡•Å‡§ï‡•á‡§∂‡§® ‡§ï‡•á ‡§∏‡§Æ‡§æ‡§™‡§® ‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡•ç‡§Æ‡§ø‡§≤‡§ø‡§§ ‡§π‡•Å‡§Ü ‡•§ ‡§á‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§ï‡•á ‡§â‡§ö‡•ç‡§ö‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä ‡§µ ‡§∏‡•ç‡§ï‡•Ç‡§≤‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§®‡•á ‡§Ö‡§™‡§®‡•Ä ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡§æ‡§à‡•§ \n",
            "Pre-processed tweet is: So ‡§Ü‡§ú ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§§‡§æ‡§≤‡§ï‡§ü‡•ã‡§∞‡§æ ‡§∏‡•ç‡§ü‡•á‡§°‡§ø‡§Ø‡§Æ ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§∏‡•å‡§ú‡§®‡•ç‡§Ø ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§π‡•à‡§™‡•Ä‡§®‡•á‡§∏ ‡§è‡§ú‡•Å‡§ï‡•á‡§∂‡§® ‡§∏‡§Æ‡§æ‡§™‡§® ‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π ‡§∏‡§Æ‡•ç‡§Æ‡§ø‡§≤‡§ø‡§§ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§â‡§ö‡•ç‡§ö‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä ‡§∏‡•ç‡§ï‡•Ç‡§≤‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡§æ‡§à\n",
            "['‡§∏‡•Å‡§Ö‡§∞ ‡§¨‡•Å‡§ñ‡§æ‡§∞‡•Ä ‡§Ö‡§¨ ‡§§‡•á‡§∞‡•á ‡§ï‡•å‡§Æ ‡§Ö‡§Ç‡§§ ‡§®‡§ø‡§ï‡§ü ‡§Ü‡§§‡•á ‡§á‡§§‡§®‡§æ ‡§∏‡§Æ‡§ù ‡§≤‡•á', '‡§Ö‡§¨ ‡§¨‡§§‡§æ‡§®‡§æ ‡§ó‡•Å‡§®‡§æ‡§π ‡§Ø‡•ã‡§ó‡§æ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§π‡§¶‡•ç‡§¶ ‡§≠‡§æ‡§à ‡§¶‡§ø‡§Æ‡§æ‡§ó ‡§™‡•ç‡§∞‡§≠‡•Å', '‡§®‡•á‡§§‡§æ ‡§ö‡§æ‡§≤‡§æ‡§ï ‡§µ‡•ã ‡§ï‡•Å‡§∞‡•ç‡§∏‡•Ä ‡§ß‡§∞‡•ç‡§Æ ‡§®‡§æ‡§Æ ‡§≤‡§°‡§º‡§µ‡§æ‡§§‡•á ‡§ú‡§®‡§§‡§æ ‡§Æ‡•Å‡§∞‡•ç‡§ñ ‡§ß‡§∞‡•ç‡§Æ ‡§®‡§æ‡§Æ ‡§Ü‡§™‡§∏ ‡§≤‡§°‡§º‡§§‡•Ä', '‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§Ö‡§∏‡§Ç‡§≠‡§µ ‡§∏‡§Ç‡§≠‡§µ ‡§¨‡§®‡§æ‡§®‡•á ‡§Æ‡§π‡§æ‡§∞‡§§ ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ú‡•Ä‡§§ ‡§ê‡§§‡§ø‡§π‡§æ‡§∏‡§ø‡§ï ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§á‡§ï‡•ç‡§ï‡•Ä‡§∏ ‡§µ‡§∞‡•ç‡§∑ ‡§¨‡§®‡§µ‡§æ‡§∏ ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∞‡§æ‡§Æ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§ï‡§æ‡§Ø‡§Æ ‡§∂‡•á‡§∑ ‡§∂‡•ç‡§∞‡•Ä', '‡§∂‡§∞‡•ç‡§Æ‡§ø‡§Ç‡§¶‡§ó‡•Ä ‡§®‡§ø‡§∞‡•Å‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§§‡•Ä ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§á‡§Ç‡§¶‡•å‡§∞ ‡§¶‡§æ‡§ó‡§º‡•Ä ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§Ü‡§ï‡§æ‡§∂ ‡§µ‡§ø‡§ú‡§Ø‡§µ‡§∞‡•ç‡§ó‡•Ä‡§Ø ‡§Ö‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§Ü‡§ö‡§∞‡§£ ‡§¨‡§æ‡§∞‡•á ‡§∏‡§µ‡§æ‡§≤ ‡§∂‡§ø‡§µ‡§∞‡§æ‡§ú ‡§ú‡•Ä ‡§Æ‡•å‡§® ‡§ó‡§Ø‡•á ‡§∂‡§ø‡§µ‡§∞‡§æ‡§ú ‡§ú‡•Ä ‡§Ü‡§ì ‡§≠‡§æ‡§ú‡§™‡§æ ‡§®‡•á‡§§‡§æ‡§ì‡§Ç ‡§ö‡§æ‡§∞‡§ø‡§§‡•ç‡§∞‡§ø‡§ï ‡§™‡§§‡§® ‡§®‡•à‡§§‡§ø‡§ï‡§§‡§æ ‡§¶‡§´‡§º‡•ç‡§® ‡§Ö‡§®‡§∂‡§® ‡§Æ‡•å‡§® ‡§∏‡§¨ ‡§ï‡§π', '‡§Æ‡•Å‡§≤‡•ç‡§≤‡•Ä ‡§§‡§ï‡§≤‡•Ä‡§´ ‡§ú‡•Ä‡§ú‡§æ ‡§â‡§™‡§∞ ‡§ö‡§æ‡§∞‡•ç‡§ú ‡§∂‡•Ä‡§ü ‡§¶‡§æ‡§ñ‡§ø‡§≤ ‡§π‡§æ ‡§á‡§∏‡§ï‡•ã ‡§π‡§∞ ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§® ‡§¨‡•á‡§ó‡•Å‡§®‡§æ‡§π ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á‡§§‡§æ ‡§¶‡•ã‡§∑‡•Ä ‡§¨‡§∏ ‡§∏‡§ø‡§∞‡•ç‡§´ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø‡§ï ‡§µ‡§æ‡§≤‡•Ä ‡§∞‡§Ç‡§°‡•Ä ‡§¨‡§¶‡§®‡§æ‡§Æ ‡§ï‡§∞‡§ï‡•á ‡§∞‡•ã‡§ü‡•Ä ‡§∂‡•á‡§ï ‡§∞‡§π‡•Ä', '‡§≤‡•ã‡§ó ‡§ê‡§∏‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§ö‡•Å‡§™ ‡§¨‡•à‡§†‡•á ‡§â‡§∏‡§ï‡§æ ‡§¨‡•â‡§Ø‡§ï‡§æ‡§ü ‡§ï‡§∞‡•ã ‡§π‡§∞‡§æ‡§Æ‡•Ä ‡§∏‡§æ‡§≤‡•á', '‡§µ‡§ø‡§™‡§ï‡•ç‡§∑ ‡§ö‡§≤‡§æ ‡§¶‡•á‡§ñ‡•ã Free voice Red', '‡§§‡•Å ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§ó‡§≤‡§§‡•Ä ‡§á‡§Ç‡§§‡§ú‡§æ‡§∞ ‡§ê‡§∏‡§æ ‡§∞‡§Ç‡§°‡•Ä ‡§ó‡•ç‡§∞‡§æ‡§π‡§ï', '‡§¨‡•Ä‡§µ‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•á ‡§∏‡•Ç‡§Ö‡§∞ ‡§ú‡§æ‡§Ø‡§¶‡§æ ‡§á‡§≤‡§æ‡§ú ‡§ï‡§∞‡§æ‡§è‡§Å‡§ó‡•á', '‡§µ‡•à‡§∏‡•á ‡§π‡•ã‡§ï‡§∞ ‡§ï‡§∞‡•á‡§ó‡§æ', '‡§Æ‡•Å‡§ù‡•á ‡§™‡•Ç‡§∞‡•á ‡§Æ‡•Å‡§≤‡•ç‡§ï‡§º ‡§∏‡§ø‡§∞‡•ç‡§´ ‡§∂‡§ñ‡•ç‡§∏ ‡§ú‡•Ä‡§§‡§®‡•á ‡§¶‡§ø‡§≤‡•Ä ‡§ñ‡•Å‡§∂‡•Ä ‡§∂‡§ñ‡•ç‡§∏ ‡§∏‡•Ä ‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶‡•á ‡§ú‡§®‡§æ‡§¨ ‡§ú‡•Ä‡§§ ‡§Æ‡•Å‡§¨‡§æ‡§∞‡§ï‡§º‡§¨‡§æ‡§¶ ‡§Ö‡§≤‡•ç‡§≤‡§æ‡§π ‡§Ü‡§™‡§ï‡•ã ‡§ñ‡•Ç‡§¨ ‡§ï‡§æ‡§Æ‡§Ø‡§æ‡§¨‡•Ä ‡§Ö‡§§‡§æ ‡§´‡§∞‡§Æ‡§æ‡§Ø‡•á ‡§Ü‡§Æ‡•Ä‡§®', '‡§Ü‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§¨‡•á‡§π‡§§‡§∞‡•Ä‡§® ‡§â‡§§‡•ç‡§§‡§∞‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§Ü‡§ú ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞', '‡§µ‡•ã ‡§π‡§∞‡§æ‡§Æ‡•Ä ‡§Æ‡§æ‡§Å ‡§ó‡§∞‡•ç‡§≠ ‡§≤‡§ú‡§º‡•ç‡§ú‡§º‡§ø‡§§ ‡§ö‡§®‡•ç‡§¶ ‡§µ‡•ã‡§ü‡•ã ‡§ï‡•ã‡§†‡•á ‡§¨‡•à‡§†‡§æ‡§®‡•á ‡§≤‡§ó‡§æ ‡§∏‡•Å‡§® ‡§ú‡§Ø‡§ö‡§Ç‡§¶ ‡§§‡•á‡§∞‡•á ‡§∏‡•Å‡§Ö‡§∞ ‡§¶‡•á‡§∂ ‡§Æ‡§æ‡§Å ‡§Ü‡§® ‡§ñ‡•á‡§≤‡§®‡•á ‡§®‡§π‡•Ä ‡§¶‡•á‡§Ç‡§ó‡•á', '‡§π‡§æ‡§• ‡§¶‡•á‡§∂ ‡§Ü‡§§‡§æ ‡§µ‡•ã ‡§Æ‡§æ‡§≤ ‡§∏‡§Æ‡§ù‡§ï‡§∞ ‡§â‡§°‡§º‡§æ‡§§‡§æ ‡§Æ‡§à ‡§§‡•á‡§∞‡§æ ‡§§‡§Ø ‡§≠‡§Ç‡§ó ‡§¨‡§ø‡§∏‡•ç‡§§‡§∞ ‡§∏‡§Æ‡•á‡§ü‡§®‡§æ ‡§∂‡•Å‡§∞‡•Ç ‡§¶‡•á', '‡§∏‡•Ç‡§Ö‡§∞ ‡§Ü‡§ï‡§æ ‡§Æ‡§∏‡•Ç‡§¶ ‡§ï‡•á‡§ú‡§º‡§∞‡•Ä ‡§ñ‡§æ‡§® ‡§°‡•á‡§≤‡•ç‡§π‡•Ä ‡§¨‡•á‡§≤‡•Ä', '‡§ú‡§æ‡§Ø‡§∞‡§æ ‡§µ‡§∏‡•Ä‡§Æ ‡§á‡§∏‡•ç‡§≤‡§æ‡§Æ ‡§π‡§µ‡§æ‡§≤‡§æ ‡§¶‡•á‡§ï‡§∞ ‡§¨‡§æ‡§ï‡•Ä ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§π‡§∞‡§æ‡§Æ‡•Ä ‡§π‡§∞‡§æ‡§Æ ‡§ñ‡§æ‡§®‡•á ‡§∏‡§ø‡§¶‡•ç‡§ß ‡§≠‡§æ‡§∞‡§§ ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§è‡§ï‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§¶‡•á‡§∂ ‡§¨‡§π‡•Å‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§ï ‡§Ö‡§≤‡•ç‡§™‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§ï ‡§¨‡§∞‡§æ‡§¨‡§∞ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§¶‡•á‡§®‡•á ‡§≠‡•Ä‡§ñ ‡§∏‡§∞‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§Æ‡§æ‡§Ç‡§ó‡§§‡§æ ‡§Æ‡§ø‡§≤‡§§‡•Ä ‡§¶‡•Å‡§§‡•ç‡§ï‡§æ‡§∞', '‡§™‡•Å‡§Ç‡§õ from the party organised by Indian Army yesterday Via', 'People voted to enjoy sufferings and they badly deserve it', '‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§ï‡§™ ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§™‡•Ä‡§õ‡§æ ‡§â‡§§‡§∞‡•Ä ‡§¨‡§æ‡§Ç‡§ó‡•ç‡§≤‡§æ‡§¶‡•á‡§∂ ‡§≤‡§ó‡•á ‡§ö‡§æ‡§∞ ‡§ù‡§ü‡§ï‡•á ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§°‡§ü‡•á ‡§∂‡§æ‡§ï‡§ø‡§¨', '‡§ö‡•á‡§∏‡•ç‡§ü‡§∞ ‡§≤‡•Ä ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§ü ‡§∞‡§ø‡§µ‡§∞‡§∏‡§æ‡§á‡§° ‡§ó‡•ç‡§∞‡§æ‡§â‡§Ç‡§° ‡§Ü‡§ú ‡§¨‡•Ä‡§ö ‡§Æ‡•Å‡§ï‡§æ‡§¨‡§≤‡§æ ‡§ñ‡•á‡§≤‡§æ ‡§ú‡§æ‡§è‡§ó‡§æ ‡§•‡•ã‡§°‡§º‡•Ä ‡§¶‡•á‡§∞ ‡§ü‡•â‡§∏ ‡§π‡•ã‡§ó‡§æ', '‡§Ö‡§≤‡•ç‡§≤‡§æ‡§π ‡§§‡•á‡§∞‡•á ‡§¨‡§æ‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§∏‡•Ç‡§Ö‡§∞ ‡§î‡§≤‡§æ‡§¶ ‡§π‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§≤‡•Ä ‡§¶‡•á‡§®‡•á ‡§Ü‡§§‡§æ', '‡§¶‡•Å‡§Ü‡§è‡§Ç ‡§π‡§∞ ‡§ú‡§ó‡§π ‡§ï‡§º‡•Å‡§¨‡•Ç‡§≤ ‡§¨‡§∂‡§∞‡•ç‡§§‡•á ‡§á‡§Ç‡§∏‡§æ‡§® ‡§Ö‡§≤‡•ç‡§≤‡§æ‡§π ‡§π‡•Å‡§ï‡•ç‡§Æ‡•ã ‡§∞‡•ã‡§ú‡§º ‡§®‡§Æ‡§æ‡§ú‡§º ‡§Ö‡§¶‡§æ', '‡§Æ‡•Å‡§ú‡§´‡•ç‡§´‡§∞‡§™‡•Å‡§∞ ‡§ú‡§ø‡§≤‡•á ‡§∏‡§π‡§ø‡§§ ‡§ï‡§∞‡•Ä‡§¨ ‡§ú‡§ø‡§≤‡•ã‡§Ç ‡§´‡•à‡§≤‡•á ‡§è‡§ï‡•ç‡§Ø‡•Ç‡§ü ‡§á‡§Ç‡§∏‡•á‡§´‡§≤‡§æ‡§á‡§ü‡§ø‡§∏ ‡§∏‡§ø‡§Ç‡§°‡•ç‡§∞‡•ã‡§Æ ‡§è‡§à‡§è‡§∏ ‡§™‡•Ä‡§°‡§º‡§ø‡§§ ‡§π‡•ã‡§ï‡§∞ ‡§Æ‡•å‡§§ ‡§Æ‡•Å‡§Ç‡§π ‡§®‡§ø‡§ï‡§≤ ‡§ö‡•Å‡§ï‡•á ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§Ö‡§¨ ‡§¶‡§ø‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ó ‡§Ü‡§∂‡§Ç‡§ï‡§æ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§ ‡§∞‡§π‡•Ä', '‡§≠‡•ã‡§∏‡§°‡§º‡•Ä ‡§ù‡§æ‡§°‡•Ç ‡§§‡•á‡§∞‡•Ä ‡§Æ‡§æ‡§Ç ‡§Ü‡§Ç‡§ñ ‡§§‡•á‡§∞‡•á ‡§¨‡§æ‡§™ ‡§ú‡§æ‡§ó‡•Ä‡§∞ ‡§π‡§ø‡§Ç‡§¶‡•Å‡§∏‡•ç‡§§‡§æ‡§® ‡§∏‡§¨‡§ï‡§æ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§® ‡§π‡§ø‡§Ç‡§¶‡•Ç ‡§¨‡§æ‡§Ç‡§ó‡•ç‡§≤‡§æ‡§¶‡•á‡§∂ ‡§π‡§ø‡§Ç‡§¶‡•Ç ‡§∞‡§π ‡§∏‡§æ‡§≤‡•á ‡§ù‡§æ‡§°‡•Ç ‡§π‡§∞‡§æ‡§Æ‡§ø ‡§î‡§≤‡§æ‡§¶ ‡§Ü‡§¶‡§Æ‡§ñ‡•ã‡§∞ ‡§î‡§≤‡§æ‡§¶ ‡§§‡•á‡§∞‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§´‡•à‡§Æ‡§ø‡§≤‡•Ä ‡§π‡§æ‡§∞‡§æ‡§Æ‡•Ä ‡§ù‡§æ‡§°‡•Ç ‡§Ö‡§ó‡§≤‡•Ä ‡§¨‡§æ‡§∞ ‡§Æ‡§§ ‡§¨‡•ã‡§≤‡§®‡§æ ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§´‡§æ‡§≤‡§§‡•Ç', '‡§ï‡§≠‡•Ä ‡§ï‡§≠‡•Ä ‡§≤‡§ó‡§§‡§æ ‡§ì‡§µ‡•à‡§∏‡•Ä ‡§®‡§æ‡§Æ ‡§¨‡•ç‡§∞‡§æ‡§Ç‡§°', 'Rss ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§∏‡•á‡§µ‡§ø‡§ï‡§æ ‡§∏‡§Æ‡§ø‡§§‡§ø ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§ï‡§æ ‡§è‡§µ‡§Æ ‡§™‡•ç‡§∞‡§•‡§Æ ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§∏‡§Ç‡§ö‡§æ‡§≤‡§ø‡§ï‡§æ ‡§µ‡§Ç‡§¶‡§®‡•Ä‡§Ø ‡§Æ‡•å‡§∂‡§ø‡§ú‡•Ä ‡§ï‡•á‡§≤‡•ç‡§ï‡§∞ ‡§ú‡•Ä ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§µ‡§∏ ‡§∏‡•á‡§µ‡§ø‡§ï‡§æ‡§ì‡§Ç ‡§∂‡§§‡•ç ‡§∂‡§§‡•ç ‡§®‡§Æ‡§®', '‡§≤‡•ã‡§ó‡•ã ‡§∏‡§¨‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§ü‡§æ‡§Å‡§ó ‡§™‡•Å‡§∞‡•á ‡§¶‡§ø‡§ñ‡§æ‡§ì', '‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ BJP ‡§∏‡•Å‡§®‡§æ‡§Æ‡•Ä ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§Æ‡•Å‡§ù‡•á ‡§§‡•á‡§∞‡•Ä ‡§Æ‡•á‡§π‡§∞‡§¨‡§æ‡§®‡§ø‡§Ø‡§æ ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§µ‡§´‡§æ‡§¶‡§æ‡§∞‡•Ä ‡§Ø‡§æ‡§¶ ‡§Ü ‡§ó‡§à', '‡§ï‡§π ‡§π‡•Å‡§ï‡•Ç‡§Æ‡§§ ‡§°‡§∞‡•á‡§Ç‡§ó‡•á ‡§ó‡§ø‡§∞ ‡§ú‡§æ‡§Ø‡•á‡§Ç‡§ó‡•á ‡§â‡§†‡•á‡§Ç‡§ó‡•á ‡§â‡§† ‡§≤‡§°‡§º‡•á‡§Ç‡§ó‡•á', '‡§¨‡§æ‡§π‡•Å‡§¨‡§≤‡•Ä ‡§π‡§ø‡§Ç‡§¶‡•Ç ‡§∏‡§Æ‡§æ‡§ú ‡§∏‡•Ä‡§¨‡•Ä‡§Ü‡§à ‡§ï‡•á‡§∏ ‡§¶‡§∞‡•ç‡§ú ‡§∞‡§π‡•Ä ‡§Ö‡§§‡•Ä‡§ï ‡§Ö‡§π‡§Æ‡§¶ ‡§™‡§∞‡•§‡§Ø‡•á ‡§™‡§ï‡•ç‡§∑‡§™‡§æ‡§§ ‡§ï‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§π‡•Å‡§¨‡§≤‡•Ä ‡§Ö‡§§‡•Ä‡§ï ‡§Ö‡§π‡§Æ‡§¶ CBI ‡§¶‡§∞‡•ç‡§ú ‡§ï‡•á‡§∏ ‡§™‡•ç‡§∞‡•â‡§™‡§∞‡•ç‡§ü‡•Ä ‡§°‡•Ä‡§≤‡§∞ ‡§ú‡•á‡§≤ ‡§™‡•Ä‡§ü‡§®‡•á ‡§Ü‡§∞‡•ã‡§™ Source OneIndia via Dailyhunt ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§°', '‡§§‡•Å‡§Æ ‡§≠‡§°‡§º‡§µ‡•á ‡§™‡§∞‡•ç‡§Ø‡§æ‡§Ø ‡§≠‡§°‡§º‡§µ‡§æ ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ', 'Part ‡§¶‡•á‡§∏‡•Ä Lola Bhabhi ‡§≠‡§æ‡§≠‡•Ä ‡§ö‡•Å‡§¶‡§æ‡§à years', '‡§≠‡§∞‡•Ä ‡§∞‡§æ‡§§ ‡§≠‡§æ‡§≠‡•Ä ‡§ö‡•ã‡§¶‡§æ ‡§â‡§∏‡§ï‡•Ä ‡§ö‡•Ç‡§§ ‡§´‡§æ‡§°‡§º ‡§ö‡•ã‡§¨‡§æ‡§∞‡§æ ‡§¨‡§®‡§æ ‡§≠‡§æ‡§¨‡•Ä ‡§≠‡•ã‡§§ ‡§Æ‡§ú‡§º‡•á ‡§¶‡•á‡§µ‡§∞ ‡§≤‡•Å‡§Ç‡§° And town For simply come and experience', '‡§≠‡•Å‡§ú‡§Ç‡§ó‡§æ‡§∏‡§® ‡§®‡§ø‡§Ø‡§Æ‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§™‡•Ä‡§† ‡§¶‡§∞‡•ç‡§¶ ‡§Ü‡§∞‡§æ‡§Æ ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§Ü‡§∏‡§® ‡§∏‡§æ‡§∞‡•á ‡§´‡§æ‡§Ø‡§¶‡•á', '‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§¶‡§ø‡§≤ ‡§§‡§æ‡§≤‡§ø‡§¨‡§æ‡§®‡•Ä ‡§∏‡•ã‡§ö ‡§™‡§æ‡§∞‡•ç‡§ï‡§ø‡§Ç‡§ó ‡§ù‡§ó‡§°‡§º‡•á ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§®‡§ø‡§∂‡§æ‡§®‡§æ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ù‡§æ‡§∞‡§ñ‡§Ç‡§° ‡§Ü‡§ï‡•ç‡§∞‡•ã‡§∂ ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ñ‡§æ‡§Æ‡•ã‡§∂ ‡§¶‡•á‡§ñ‡§ø‡§è ‡§∂‡§æ‡§Æ ‡§¨‡§ú‡•á', '‡§ï‡§∞‡•ç‡§Æ‡•ã ‡§ï‡§æ‡§∞‡§£', '‡§Æ‡•ã‡§π‡§Æ‡•ç‡§Æ‡§¶ ‡§∂‡§Æ‡•Ä ‡§∏‡§™‡§æ ‡§¨‡§∏‡§™‡§æ ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§ú‡§æ‡§Ø‡•á ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§∏‡§æ‡§¨‡§ø‡§§ ‡§¶‡•á‡§ó‡•Ä ‡§Ö‡§´‡§ó‡§æ‡§®‡§ø‡§∏‡•ç‡§§‡§æ‡§® ‡§Æ‡•à‡§ö ‡§∂‡§æ‡§Æ‡•Ä ‡§µ‡§ø‡§ï‡•á‡§ü ‡§µ‡•ã ‡§ó‡•á‡§Ç‡§¶‡•á ‡§µ‡§æ‡§à‡§° ‡§¨‡•â‡§≤ ‡§•‡•Ä‡§Ç', '‡§§‡•Ç ‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶ ‡§Æ‡§æ‡§Å ‡§≠‡•ã ‡§¶‡•á‡§ñ ‡§∏‡•Å‡§Ö‡§∞ ‡§ö‡•ã‡§¶‡•á ‡§™‡§§‡•ç‡§∞ ‡§ï‡§æ‡§∞ ‡§ù‡§æ‡§Ç‡§ü ‡§≠‡•ã‡§∑‡§°‡§º‡•Ä ‡§§‡•á‡§∞‡§æ ‡§ñ‡§æ‡§®‡§¶‡§æ‡§® ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§π‡•Å‡§ï‡•Ç‡§Æ‡§§ ‡§≤‡•ã‡§π‡§æ ‡§≤‡§ø‡§Ø‡§æ', '‡§Æ‡§§‡§≤‡§¨ ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§Ö‡§™‡•Ä‡§≤ ‡§µ‡•ã ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§µ‡•ã‡§ü ‡§¶‡•á‡§Ç ‡§ñ‡•Å‡§ú‡§≤‡•Ä‡§µ‡§æ‡§≤', 'Every soul will taste of death And ye will be paid on the Day of Resurrection only that which ye have fairly earned Who so is removed from the Fire and is made to enter Paradise he indeed is triumphant The life of this world is but comfort of illusion Quran', '‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§∞‡§Ç‡§°‡•Ä ‡§¨‡§Ç‡§ö‡•ç‡§ö‡§æ ‡§ï‡§µ‡§ø‡§§‡§æ ‡§∏‡•Å‡§®‡§æ ‡§ú‡§ø‡§∏‡§∏‡•á ‡§Æ‡§æ‡§π‡•å‡§≤ ‡§ó‡§∞‡§Æ ‡§®‡•ç‡§Ø‡•Ç‡§ú ‡§ö‡•à‡§®‡§≤‡•ã ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß ‡§π‡§∞‡§æ‡§Æ‡•Ä ‡§™‡•Å‡§∞‡•Ä ‡§ï‡§µ‡§ø‡§§‡§æ ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§ú‡§æ‡§Ø‡•á', '‡§ö‡•Å‡§®‡§æ‡§µ ‡§≤‡§°‡§º‡•á ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§Ü‡§™‡§§‡•ç‡§§‡§ø ‡§¨‡§æ‡§∞ ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§¨‡§æ‡§∞ ‡§∏‡§æ‡§Ç‡§∏‡§¶ ‡§∞‡§π‡§®‡•á ‡§¨‡§æ‡§µ‡§ú‡•Ç‡§¶ ‡§Ö‡§§‡•Ä‡§ï ‡§Ö‡§π‡§Æ‡§¶ ‡§®‡§æ‡§Æ ‡§Æ‡§æ‡§´‡§ø‡§Ø‡§æ ‡§¨‡§æ‡§π‡•Å‡§¨‡§≤‡•Ä ‡§ú‡§æ‡§®‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§™‡•ç‡§∞‡§æ‡§á‡§Æ ‡§ü‡§æ‡§á‡§Æ ‡§°‡•â‡§® ‡§ï‡§π‡§ï‡§∞ ‡§õ‡§µ‡§ø ‡§¨‡§ø‡§ó‡§æ‡§°‡§º‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§ó‡§∞‡•Ä‡§¨‡•ã ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§â‡§†‡§æ‡§®‡§æ ‡§Æ‡§¶‡§¶ ‡§ó‡•Å‡§£‡•ç‡§°‡§æ‡§ó‡§∞‡•ç‡§¶‡•Ä', '‡§ó‡•Å‡§≤‡§æ‡§¨‡•Ä ‡§ï‡§Ç‡§π‡§æ ‡§≠‡§°‡§º‡§µ‡§æ sorry ‡§≠‡§ó‡§µ‡§æ ‡§ó‡•à‡§Ç‡§ó ‡§ó‡•Å‡§≤‡§æ‡§Æ‡•Ä ‡§ó‡•à‡§Ç‡§ó part', '‡§ú‡•Ä‡§ú‡§æ ‡§∏‡§æ‡§≤‡•Ä ‡§ó‡§∞‡§Æ ‡§ö‡•Å‡§¶‡§æ‡§à sec', '‡§Ö‡§¨ ‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§π‡§∞ ‡§¨‡§æ‡§§ ‡§∏‡§π‡•Ä ‡§ó‡§º‡§≤‡§§ ‡§§‡•ã‡§≤‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§∏‡§¨ ‡§ú‡§ø‡§Ç‡§¶‡§ó‡§æ‡§®‡•Ä ‡§ö‡§≤‡•á ‡§ó‡§Ø‡•á ‡§ï‡§π‡§æ‡§®‡•Ä ‡§¨‡§æ‡§§‡•á ‡§Ø‡•Ç‡§Ç ‡§π‡•Ä‡§Ç ‡§ö‡§≤‡§§‡•Ä ‡§∞‡§π‡•á‡§Ç‡§ó‡•Ä ‡§§‡•Å‡§ù‡§∏‡•á ‡§ï‡§∞‡§§‡•Ä ‡§µ‡•ã ‡§ñ‡•Å‡§¶ ‡§ï‡§∞‡•á‡§Ç‡§ó‡•Ä', '‡§ú‡§Æ‡•ç‡§Æ‡•Ç ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§®‡•á‡§§‡§æ ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§∏‡•á‡§®‡§æ ‡§ú‡§Æ‡•Ä‡§® ‡§ï‡§¨‡•ç‡§ú‡§æ ‡§Æ‡§§‡§≤‡§¨ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§π‡§ø‡§Ç‡§¶‡•Ç‡§§‡•ç‡§µ ‡§ñ‡§§‡§∞‡•á', '‡§ï‡•ç‡§≤‡§æ‡§∏‡§Æ‡•á‡§ü ‡§¶‡•Ä‡§™‡§ø‡§ï‡§æ ‡§®‡§ø‡§Ø‡§§ ‡§¨‡§ø‡§ó‡§°‡§º ‡§¶‡•Ä ‡§Æ‡•á‡§∞‡•á ‡§≤‡•Å‡§Ç‡§°', '‡§ï‡•ç‡§Ø‡§æ ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø ‡§®‡•å‡§ú‡§µ‡§æ‡§® ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§¨‡§®‡§ï‡•á ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§≤‡§°‡§º‡§ï‡•á ‡§™‡§ï‡§°‡§º‡§ï‡•á ‡§•‡§™‡•ç‡§™‡§°‡§º ‡§Æ‡§æ‡§∞‡•ç‡§ï‡•á ‡§ú‡§Ø ‡§∂‡•ç‡§∞‡•Ä‡§∞‡§æ‡§Æ ‡§¨‡•ã‡§≤‡§®‡•á ‡§Æ‡§ú‡§¨‡•Ç‡§∞ ‡§®‡§π‡•Ä ‡§ï‡§∞‡§æ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§∏‡§Æ‡§æ‡§ú ‡§¨‡§¶‡§®‡§æ‡§Æ ‡§á‡§∏‡§Æ‡•á ‡§™‡•ã‡§≤‡§ø‡§ü‡§ø‡§ï‡§≤ ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§π‡§æ‡§• ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§¨‡§∞‡•ç‡§¨‡§æ‡§¶ ‡§∏‡§æ‡§µ‡§ß‡§æ‡§®', '‡§Ö‡§¨ ‡§ú‡§®‡§§‡§æ ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§ë‡§ü‡•ã‡§Æ‡•à‡§ü‡§ø‡§ï ‡§¨‡§æ‡§∞‡•Ç‡§¶ ‡§ï‡§∞‡§ï‡•á ‡§¨‡§®‡•ç‡§¶ ‡§ï‡§∞‡•ã‡§ó‡•á ‡§ñ‡§ø‡§≤‡•å‡§®‡§æ ‡§ö‡§≤‡§æ‡§è‡§Ç‡§ó‡•á ‡§á‡§¨ ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§ï‡§∞‡•ç‡§Æ ‡§ï‡§∞‡•ã‡§ó‡•á ‡§´‡•á‡§∞ ‡§Æ‡•å‡§§ ‡§Æ‡§∞‡•ã‡§ó‡•á ‡§¨‡•Ä ‡§ü‡•á‡§Æ ‡§Ü‡§ó‡•ç‡§Ø‡§æ', '‡§≠‡§ó‡§µ‡§æ ‡§ú‡§∞‡•ç‡§∏‡•Ä ‡§¶‡•á‡§ñ ‡§≤‡§ó ‡§¨‡§æ‡§∞ ‡§ï‡§Æ‡§æ‡§® ‡§ú‡•Ä ‡§π‡§æ‡§•', '‡§∏‡§ø‡§ñ‡•ã‡§Ç ‡§π‡§§‡•ç‡§Ø‡§æ‡§∞‡§æ ‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶ ‡§ï‡§æ‡§≤‡•á ‡§≤‡•Å‡§Ç‡§° ‡§™‡•à‡§¶‡§æ‡§à‡§∂', '‡§Ö‡§≤‡•ç‡§≤‡§π‡§æ ‡§π‡§∞‡§æ‡§Æ ‡§π‡§∞‡§æ‡§Æ‡•Ä', '‡§ú‡§æ‡§§‡§ø‡§Ø‡•ã‡§Ç sc ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§Ü‡§∞‡§ï‡•ç‡§∑‡§£ ‡§Ö‡§®‡•Å‡§™‡§æ‡§§ ‡§¨‡§¢‡§º‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è ‡§∏‡§π‡§Æ‡§§ retweet ‡§ï‡§∞‡•ã', '‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§∏‡•Ç‡§Ö‡§∞ ‡§Ö‡§∏‡§≤‡§ø‡§Ø‡§§ ‡§™‡§§‡§æ ‡§ö‡§≤‡•Ä ‡§¨‡•Å‡§∞‡§æ ‡§≤‡§ó‡§æ', '‡§Æ‡§Ø‡§Ç‡§ï ‡§ó‡•Å‡§™‡•ç‡§§‡§æ ‡§≠‡§°‡§º‡§µ‡§æ ‡§¶‡§≤‡§æ‡§≤ ‡§ú‡§≤‡•á ‡§ú‡§≤‡•á', '‡§ï‡§≤ ‡§∞‡§æ‡§§ ‡§∏‡§™‡§®‡•á ‡§≤‡§°‡§º‡§ï‡•Ä ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§¨‡§∞‡•ç‡§§‡§® ‡§Æ‡§Ç‡§ú‡§µ‡§æ‡§è ‡§ï‡§™‡§°‡§º‡•á ‡§ß‡•Å‡§≤‡§µ‡§æ‡§è ‡§ï‡•ç‡§Ø‡§æ ‡§®‡§ø‡§ï‡§ü ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§∂‡§æ‡§¶‡•Ä ‡§∏‡§Ç‡§ï‡•á‡§§ ‡§∏‡§Æ‡§ù‡•Ç‡§Ç ‡§ï‡•É‡§™‡§Ø‡§æ ‡§∂‡§æ‡§¶‡•Ä‡§∂‡•Å‡§¶‡§æ ‡§≤‡•ã‡§ó ‡§Æ‡§æ‡§∞‡•ç‡§ó‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§∞‡•á', 'Kya Akaless ji post to aap ayse kar rage ho jaise aap ne apne salo me kafi teer mare hoKuch to saram karo', '‡§Ö‡§¨‡•á ‡§ï‡§ü‡•á ‡§≤‡§Ç‡§° ‡§î‡§≤‡§æ‡§¶ ‡§∞‡§Ç‡§°‡•Ä ‡§ñ‡§æ‡§§‡•Ç‡§® ‡§™‡•à‡§¶‡§æ‡§à‡§∏ ‡§∏‡•Å‡§® ‡§ó‡§≤‡§§ ‡§ó‡§≤‡§§ ‡§Æ‡§æ‡§® ‡§Æ‡§∞‡•ç‡§Ø‡§æ‡§¶‡§ø‡§§ ‡§≠‡§æ‡§∑‡§æ ‡§ü‡§ø‡§™‡•ç‡§™‡§£‡•Ä ‡§ï‡§∞‡•ã', '‡§§‡•Å ‡§∏‡§π‡•Ä ‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶ ‡§î‡§≤‡§æ‡§¶ ‡§ï‡•Å‡§§‡•ç‡§§‡•á', '‡§Ü‡§ú ‡§®‡§æ‡§π‡§ø‡§¶ ‡§à‡§¶ ‡§¨‡§ß‡§æ‡§à ‡§¶‡•á‡§§‡•á ‡§Æ‡§® ‡§≠‡•Ä‡§ó ‡§®‡§æ‡§π‡§ø‡§¶ ‡§Æ‡§æ‡§§‡§æ ‡§™‡§ø‡§§‡§æ ‡§™‡§ø‡§õ‡§≤‡•á ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§π‡§æ‡§¶‡§∏‡•á ‡§¶‡•Å‡§ñ‡§¶ ‡§®‡§ø‡§ß‡§® ‡§ï‡•â‡§≤‡•á‡§ú ‡§™‡•ç‡§∞‡§•‡§Æ ‡§µ‡§∞‡•ç‡§∑ ‡§™‡§¢‡§º‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§™‡•à‡§∞ ‡§ó‡§Ç‡§µ‡§æ‡§®‡§æ ‡§™‡§°‡§º‡§æ ‡§Æ‡•à‡§Ç‡§®‡•á ‡§™‡§¢‡§º‡§æ‡§à ‡§≤‡§ø‡§ñ‡§æ‡§à ‡§π‡§∞ ‡§∏‡§Ç‡§≠‡§µ ‡§Æ‡§¶‡§¶ ‡§µ‡§æ‡§¶‡§æ ‡§à‡§¶ ‡§Æ‡•Å‡§¨‡§æ‡§∞‡§ï ‡§¨‡§ø‡§ü‡§ø‡§Ø‡§æ', '‡§∏‡§æ‡§≤ ‡§¨‡§¶‡§≤‡§æ ‡§¨‡§¶‡§≤‡•Ä ‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞ ‡§∏‡•Å‡§®‡§ø‡§è ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§π‡§®‡§æ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§™‡•ç‡§∞‡§µ‡§ï‡•ç‡§§‡§æ Live', '‡§ú‡§æ‡§®‡§ø‡§è ‡§¨‡§ú‡§ü ‡§ï‡•à‡§∏‡§æ ‡§∞‡§π‡•á‡§ó‡§æ ‡§∂‡•á‡§Ø‡§∞ ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§π‡§æ‡§≤ ‡§ï‡•ç‡§Ø‡§æ ‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§ü‡•á‡§ú‡•Ä ‡§¨‡§®‡§æ ‡§®‡§ø‡§µ‡•á‡§∂‡§ï', '‡§¶‡•á‡§ñ‡•ã ‡§π‡§Æ‡§æ‡§∞‡•Ä ‡§¶‡•Ä‡§¶‡•Ä ‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§ö‡§≤‡§æ‡§§‡•á ‡§∞‡§æ‡§π‡•Å‡§≤ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§¶‡§ø‡§ñ ‡§ó‡§è ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§Æ‡•å‡§§ ‡§ñ‡§æ‡§Æ‡•ã‡§∏ ‡§Æ‡•ã‡§¶‡•Ä ‡§®‡§π‡•Ä ‡§¶‡§ø‡§ñ‡•á ‡§™‡§§‡•ç‡§∞‡§ï‡§æ‡§∞‡•ã‡§Ç PMO ‡§ë‡§´‡§ø‡§∏ ‡§ò‡§Ç‡§ü‡•á ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§¶‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§µ‡§ø‡§™‡§ï‡•ç‡§∑ ‡§õ‡•ã‡§ü‡•Ä ‡§õ‡•ã‡§ü‡•Ä ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§¨‡§§‡§Ç‡§ó‡§°‡§º ‡§¨‡§®‡§æ‡§®‡•á ‡§≤‡§ó‡•Ä ‡§∞‡§π‡•ã ‡§¨‡§π‡§ø‡§Ç‡§® ‡§ú‡•Ä ‡§¶‡•á‡§∂‡§π‡§ø‡§§ ‡§¨‡§∞‡•ç‡§¨‡§æ‡§¶ ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§Ø‡§æ‡§¶ ‡§ú‡§æ‡§è‡§ó‡§æ', '‡§¶‡•á‡§∂ ‡§™‡§¢‡§º‡§æ ‡§≤‡§ø‡§ñ‡§æ ‡§Ø‡•Å‡§µ‡§æ ‡§π‡§æ‡§∞ ‡§ú‡§æ‡§è ‡§ú‡•Ä‡§§ ‡§ú‡§æ‡§Ø‡•á ‡§¶‡•á‡§∂ ‡§ú‡§®‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ', '‡§Ü‡§§‡§Ç‡§ï‡•Ä ‡§¶‡•á‡§∂‡§≠‡§ï‡•ç‡§§ ‡§¨‡§§‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§µ‡§æ‡§π‡•Ä ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á ‡§µ‡•ã Hidden Agenda ‡§™‡§∞‡•ç‡§¶‡§æ ‡§°‡§æ‡§≤ ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á ‡§¨‡§¶‡§≤ ‡§¶‡•á‡§Ç‡§ó‡•á', '‡§ú‡§®‡§§‡§æ ‡§´‡•à‡§∏‡§≤‡§æ ‡§∏‡•Å‡§®‡§æ ‡§π‡§Æ‡•á ‡§ö‡§æ‡§π‡§ø‡§è ‡§®‡§π‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§¶‡•á‡§∂ ‡§ü‡•Å‡§ï‡§°‡§º‡•á ‡§ï‡§∞‡•á ‡§ú‡§ø‡§∏‡§ï‡•á ‡§ä‡§™‡§∞ ‡§Ü‡§∞‡•ã‡§™ ‡§µ‡•ã‡§ü ‡§¶‡•á‡§ó‡§æ', 'Dineout Now shop till you drop Me ‡§≠‡§ï ‡§≠‡•ã‡§∏‡§°‡•Ä‡§ï‡•á', '‡§Æ‡•ã‡§¶‡•Ä ‡§ú‡•Ä ‡§ï‡§ø‡§∞‡§¶‡§æ‡§∞ ‡§®‡§ø‡§≠‡§æ‡§§‡•á ‡§®‡§ø‡§≠‡§æ‡§§‡•á ‡§Æ‡•ã‡§¶‡•Ä ‡§ú‡•Ä ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ò‡•Å‡§∏ ‡§ó‡§à ‡§á‡§®‡§Æ‡•á‡§Ç', 'president jb sb Ke hukum se Jharkhand Pradesh Adhyaksh sahab ne apni team ke sath Marhoom ke Ghar unki walida aur unki Bewa se Mulaqat karke rs ki madad di Aur tabrez ke liye insaf kibladai ka wada Kiya', '‡§â‡§¶‡§æ‡§∏ ‡§â‡§¶‡§æ‡§∏ ‡§∞‡§π‡§§‡§æ ‡§¶‡§ø‡§® ‡§´‡•á‡§∏‡§¨‡•Å‡§ï ‡§Ü‡§™‡§ï‡•á ‡§™‡•ã‡§∏‡•ç‡§ü ‡§™‡•ù‡§æ ‡§π‡§Æ‡•á‡§∂‡§æ ‡§ñ‡•Å‡§∂ ‡§∞‡§π‡§®‡•á ‡§≤‡§ó‡§æ ‡§Ü‡§™‡§ï‡•á ‡§™‡•ã‡§∏‡•ç‡§ü ‡§™‡•ù ‡§≤‡§æ‡§§‡•á ‡§á‡§§‡§®‡§æ ‡§ú‡•ç‡§û‡§æ‡§®', '‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§ú‡•ç‡§û‡§æ‡§® ‡§¶‡•á‡§∂ ‡§Ö‡§∞‡•ç‡§•‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ ‡§Æ‡§ú‡§¨‡•Ç‡§§‡•Ä ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§ú‡§æ‡§è‡§ó‡§æ ‡§∂‡•ã‡§ß ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§Ö‡§Ç‡§§‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§Æ‡§π‡§§‡•ç‡§µ ‡§π‡•ã‡§ó‡§æ ‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§∂‡§æ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§Ø‡•ã‡§ú‡§®‡§æ‡§è‡§Ç ‡§¶‡•á‡§∂ ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§â‡§§‡•ç‡§™‡•ç‡§∞‡•á‡§∞‡§ï ‡§≠‡•Ç‡§Æ‡§ø‡§ï‡§æ ‡§®‡§ø‡§≠‡§æ‡§è‡§Ç‡§ó‡•Ä', '‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§™‡•á‡§°‡§º ‡§ó‡§ø‡§∞ ‡§ú‡§æ‡§è ‡§ö‡§æ‡§∞ ‡§õ‡•á ‡§™‡§§‡•ç‡§§‡•á ‡§¨‡§ø‡§ñ‡§∞ ‡§ú‡§æ‡§è ‡§¶‡§≤‡•ç‡§≤‡•Ä ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§™‡§π‡•Å‡§Å‡§ö ‡§®‡§Ç‡§ó‡§æ ‡§®‡§æ‡§ö ‡§∏‡•Å‡§∞‡•Å ‡§¶‡•á‡§§‡•Ä ‡§Æ‡§Æ‡§§‡§æ ‡§®‡§π‡•Ä ‡§∏‡§Æ‡•ç‡§π‡§≤ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§π‡§æ‡§≤ ‡§¨‡•á‡§π‡§æ‡§≤ ‡§π‡•á‡§°‡§≤‡§æ‡§á‡§® ‡§¨‡§®‡§§‡•á ‡§µ‡§π‡•Ä UP ‡§¶‡§ø‡§®‡•ã ‡§≤‡§ó‡§æ‡§§‡§æ‡§∞ ‡§π‡§§‡•ç‡§Ø‡§æ ‡§¨‡§≤‡§æ‡§§‡•ç‡§ï‡§æ‡§∞ ‡§á‡§∏‡§™‡•á ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ ‡§ö‡•Å‡§™ UP ‡§Ø‡•Ç‡§ó‡§æ‡§Ç‡§°‡§æ', '‡§ú‡•Ä‡§§ ‡§∂‡§∞‡•ç‡§Æ‡§®‡§æ‡§ï ‡§ú‡§ø‡§∏‡§®‡•á ‡§∂‡§π‡§æ‡§¶‡§§ ‡§∏‡•Ç‡§§‡§ï ‡§∂‡•ç‡§∞‡§æ‡§™ ‡§Æ‡•å‡§§ ‡§ó‡•ã‡§°‡§∏‡•á ‡§¶‡•á‡§∂‡§≠‡§ï‡•ç‡§§ ‡§ú‡§¨‡§ï‡§ø ‡§¶‡•Ç‡§∏‡§∞‡•Ä ‡§§‡§∞‡§´ ‡§™‡§æ‡§ï ‡§ú‡§®‡§§‡§æ ‡§π‡§æ‡§´‡§ø‡§ú ‡§∏‡§à‡§¶ ‡§ú‡§Æ‡§æ‡§®‡§§ ‡§ú‡§¨‡•ç‡§§ ‡§ï‡§∞‡§æ ‡§¶‡•Ä ‡§õ‡•ã‡§°‡§º‡§ø‡§è ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡§ø‡§∏‡§ï‡•Ä ‡§®‡§π‡•Ä ‡§π‡§Æ‡§æ‡§∞‡•Ä ‡§¨‡•å‡§¶‡•ç‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§™‡§¶‡§æ ‡§®‡§∑‡•ç‡§ü ‡§ó‡§à ‡§ù‡•Ç‡§† ‡§õ‡§≤ ‡§ï‡§™‡§ü ‡§Ö‡§™‡§∂‡§¨‡•ç‡§¶ ‡§®‡•ç‡§Ø‡•Ç ‡§á‡§Ç‡§°‡§ø‡§Ø‡§æ ‡§¨‡§ß‡§æ‡§à ‡§Ü‡§™‡§ï‡•ã', '‡§≠‡•ã‡§∏‡§°‡•Ä‡§ï‡•á ‡§´‡•ç‡§∞‡•Ä‡§°‡§Æ ‡§ë‡§´‡§º ‡§∏‡•ç‡§™‡•Ä‡§ö ‡§ö‡§æ‡§ü‡•Å‡§ï‡§æ‡§∞ ‡§Ü‡§ú ‡§Ü‡§ó ‡§≤‡§ó‡•Ä ‡§á‡§®‡§ï‡•á ‡§™‡§ø‡§õ‡§µ‡§æ‡§°‡§º‡•á ‡§Ö‡§¨ ‡§§‡•Å‡§Æ‡•ç‡§π‡•á ‡§¨‡§∞‡•ç‡§®‡•ã‡§≤ ‡§®‡§π‡•Ä ‡§µ‡•à‡§∏‡§≤‡•Ä‡§® ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§™‡§°‡§º‡•á‡§ó‡•Ä', '‡§Æ‡§æ‡§®‡§®‡•Ä‡§Ø ‡§∂‡•ç‡§∞‡•Ä ‡§ú‡•Ä ‡§∞‡§ï‡•ç‡§∑‡§æ ‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§¨‡§®‡§®‡•á ‡§π‡§æ‡§∞‡•ç‡§¶‡§ø‡§ï ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Ç', '‡§∂‡•ç‡§∞‡•Ä‡§®‡§ó‡§∞ ‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§™‡•ç‡§∞‡§≠‡§æ‡§§ ‡§´‡•á‡§∞‡•Ä ‡§µ‡§∞‡•ç‡§∑‡•ã‡§Ç ‡§™‡§π‡§≤‡•Ä ‡§¨‡§æ‡§∞ ‡§∏‡•ã‡§ö‡•ã ‡§Ü‡§Å‡§ñ‡•á ‡§ñ‡•ã‡§≤‡•ã ‡§ï‡•à‡§∏‡•á ‡§Æ‡•Å‡§Æ‡§ï‡§ø‡§®', '‡§ú‡•ã‡§Ø‡§æ ‡§Ö‡§®‡•Å‡§™‡§Æ ‡§Ö‡§®‡•Å‡§∞‡§æ‡§ó ‡§ï‡§∂‡•ç‡§Ø‡§™ ‡§∏‡§Æ‡•á‡§§ ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§Æ‡§ø‡§≤‡§æ ‡§®‡§à ‡§ë‡§∏‡•ç‡§ï‡§∞ ‡§Ö‡§ï‡§æ‡§¶‡§Æ‡•Ä ‡§∏‡§¶‡§∏‡•ç‡§Ø ‡§¨‡§®‡§®‡•á ‡§®‡§ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡§£', '‡§∏‡•ç‡§ï‡•Ç‡§ü‡§∞ ‡§ñ‡§°‡§º‡•Ä ‡§¨‡§π‡§∏ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§π‡§æ‡§•‡§æ‡§™‡§æ‡§à ‡§π‡§≤‡•ç‡§ï‡•á ‡§ú‡§ñ‡•ç‡§Æ‡•Ä ‡§∏‡§¨ ‡§†‡•Ä‡§ï ‡§Æ‡•Å‡§π‡§≤‡•ç‡§≤‡•á ‡§¨‡§æ‡§§ ‡§™‡§∞‡•á‡§∂‡§æ‡§®‡•Ä ‡§π‡§æ‡§Ç ‡§Ö‡§≤‡§ó‡§æ‡§µ‡§¨‡§æ‡§¶‡•Ä ‡§∏‡•Å‡§Ö‡§∞ ‡§™‡§ø‡§≤‡•ç‡§≤‡•á ‡§∏‡§Æ‡§æ‡§ú ‡§¶‡•Å‡§∂‡•ç‡§Æ‡§®‡•ã‡§Ç ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§Ü‡§è‡§Ç', '‡§¨‡•ç‡§≤‡•â‡§ï ‡§ï‡•ç‡§Ø‡•Ç‡§Å ‡§≠‡•ã‡§∏‡§°‡•Ä ‡§ï‡§æ‡§ó‡§º‡§ú‡§º‡•Ä ‡§∂‡•á‡§∞ ‡§ú‡§º‡§∞‡§æ ‡§Ö‡§®‡§¨‡•ç‡§≤‡•â‡§ï ‡§ï‡§∞‡•ã ‡§¶‡§ø‡§ñ‡§§‡§æ ‡§π‡•Å‡§Å ‡§§‡•Å‡§Æ‡§ï‡•ã ‡§ï‡§ø‡§∏‡§ï‡•Ä ‡§ó‡§æ‡§Ç‡§° ‡§¶‡§Æ ‡§ö‡•Å‡§§‡§ø‡§Ø‡•á ‡§ï‡§æ‡§ó‡§º‡§ú‡§º‡•Ä ‡§π‡§ø‡§Ç‡§¶‡•Ç', '‡§Ö‡§Ç‡§ú‡§®‡§æ ‡§∞‡•Å‡§¨‡§ø‡§ï‡§æ ‡§∏‡§∞‡§¶‡§æ‡§®‡§æ ‡§≤‡§ø‡§Ç‡§ö‡§ø‡§Ç‡§ó ‡§ñ‡§¨‡§∞ ‡§§‡•Å‡§≤‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ú‡§æ‡§Ø‡§ú‡§º ‡§†‡§π‡§∞‡§æ‡§®‡§æ ‡§Ö‡§¨ ‡§∏‡§¨ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§Æ‡§æ‡§à ‡§¨‡§æ‡§™ ‡§¶‡§æ‡§Æ‡§® ‡§≤‡§ó‡§æ ‡§ñ‡•Ç‡§® ‡§ß‡§¨‡•ç‡§¨‡§æ ‡§ö‡§æ‡§ü‡§®‡•á ‡§ú‡§æ‡§§‡§æ Via', '‡§¨‡•á‡§∂‡§∞‡•ç‡§Æ‡•Ä ‡§§‡•Å‡§Æ ‡§¶‡§ø‡§ñ‡§æ‡§§‡•á ‡§π‡§∞‡§æ‡§Æ‡§ñ‡•ã‡§∞ ‡§°‡§∞‡§æ ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§® ‡§∏‡•å ‡§∏‡§æ‡§≤ ‡§™‡•Å‡§∞‡§æ‡§®‡•Ä ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§§‡•ã‡§°‡§º‡§´‡•ã‡§°‡§º ‡§π‡§∞‡§æ‡§Æ‡•Ä ‡§´‡§∞‡•ç‡§ú‡•Ä‡§µ‡§æ‡§≤ ‡§ü‡•Ä‡§Æ ‡§ñ‡§æ‡§Æ‡•ã‡§∂ ‡§â‡§∏‡§∏‡•á ‡§∏‡§µ‡§æ‡§≤ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§™‡•Ç‡§õ‡§§‡•á', '‡§≤‡•á ‡§≤‡§ø‡§Ø‡§æ ‡§Æ‡•Å‡§Ç‡§π ‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶ ‡§®‡§ø‡§ï‡§≤ ‡§≠‡§°‡§º‡§µ‡•á', '‡§µ‡§æ‡§¶‡§æ ‡§≤‡§ó‡§§‡§æ ‡§¶‡§Æ‡•á ‡§∞‡•ã‡§ó ‡§≤‡§ó ‡§∏‡•Ä‡§®‡•á', '‡§∞‡§æ‡§§‡§Æ‡§æ ‡§§ ‡§´‡•Ç‡§≤‡§≤‡§æ‡§à ‡§™‡§®‡§ø ‡§π‡•Å‡§Å‡§¶‡•à‡§® ‡§¨‡§ø‡§π‡§æ‡§® ‡§Æ‡§®‡•ç‡§¶‡§ø‡§∞ ‡§ú‡§æ‡§®‡•á ‡§ö‡§ø‡§π‡§æ‡§®', 'FB‡§™‡•ã‡§∏‡•ç‡§ü ‡§™‡§¢‡•á‡§Ç ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§ò‡§ü‡•Ä‡§Ø‡§æ ‡§∏‡•ã‡§Ç‡§ö ABVP ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§∞‡•ç‡§§‡§æ ‡§Æ‡§π‡§ø‡§≤‡§æ‡§ì‡§Ç ‡§π‡§Æ‡•á‡§∂‡§æ ‡§ó‡§≤‡§§ ‡§®‡§ú‡§∞‡§ø‡§è ‡§¶‡•á‡§ñ‡§§‡•á ‡§¨‡§∏ ‡§ö‡§≤‡•á ‡§∏‡§ó‡•Ä ‡§¨‡§π‡§® ‡§õ‡•ã‡§°‡§§‡•á ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§µ‡§ø‡§∞‡•ã‡§ß‡•Ä RSS ‡§ï‡•ã‡§∞‡•ç‡§∏ ‡§≠‡§ï‡•ç‡§§ ‡§ï‡•ã‡§∞‡•ç‡§∏ ‡§™‡•Ç‡§∞‡•Ä ‡§≠‡§æ‡§ú‡§™‡§æ ‡§®‡•á‡§§‡§æ ‡§¨‡§®‡•á ‡§¨‡•à‡§†‡•á', 'Akhilesh bhaiya ab app aha kaam kar rahe hai isi tarah lage rahiye me cm aapko banana haiJai samajvad', '‡§ó‡•Å‡§ú‡§∞‡§æ‡§§ ‡§∏‡§æ‡§≤ ‡§≠‡§æ‡§ú‡§™‡§æ ‡§®‡•á‡§§‡§æ ‡§∏‡§æ‡§≤ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§∞‡•ç‡§§‡§æ ‡§∞‡§ö‡§æ‡§à ‡§§‡•Ä‡§∏‡§∞‡•Ä ‡§∂‡§æ‡§¶‡•Ä ‡§Æ‡•ã‡§¶‡•Ä ‡§ú‡•Ä ‡§π‡§≤‡§æ‡§≤‡§æ ‡§§‡•Ä‡§® ‡§§‡§≤‡§æ‡§ï‡§º ‡§®‡§§‡§æ‡§ì ‡§∏‡§Ç‡§≠‡§æ‡§≤‡§ø‡§è', '‡§≠‡§ó‡§µ‡§æ ‡§¨‡•ç‡§∞‡§ø‡§ó‡•á‡§° ‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§®‡•ã ‡§ñ‡§§‡•ç‡§Æ ‡§ö‡§æ‡§π‡§§‡•Ä ‡§≠‡§æ‡§ú‡§™‡§æ ‡§ö‡§æ‡§π‡§§‡•Ä ‡§Ø‡•Å‡§µ‡§æ ‡§™‡§¢‡§º‡•á ‡§®‡§π‡•Ä ‡§™‡§ï‡•ã‡§°‡§º‡•á ‡§¨‡•á‡§ö‡•á ‡§ú‡•á‡§®‡§Ø‡•Ç ‡§Ö‡§¨ ‡§∏‡•Å‡§®‡§ø‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§§‡§∞‡•Ä‡§ï‡•á ‡§Ö‡§≤‡•Ä‡§ó‡§¢‡§º ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§Ø‡•Ç‡§®‡§ø‡§µ‡§∞‡•ç‡§∏‡§ø‡§ü‡•Ä ‡§Æ‡§æ‡§π‡•å‡§≤ ‡§ñ‡§∞‡§æ‡§¨ ‡§ê‡§∏‡•Ä ‡§∏‡•ç‡§ü‡•Ç‡§°‡•á‡§Ç‡§ü‡•ç‡§∏ ‡§µ‡§ø‡§∞‡•ã‡§ß‡•Ä ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§∞‡•ã‡§ß‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§π‡§Æ ‡§∏‡§§‡•ç‡§§‡§æ ‡§π‡§ü‡§æ‡§ï‡§∞ ‡§∞‡§π‡•á‡§Ç‡§ó‡•á', 'Musalman ki majboori hai Congress jaise hi musalmano ko option milega musalman Congress ko chordega Spokesperson on', '‡§ö‡•Å‡§®‡§æ‡§µ ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ñ‡•Å‡§≤‡•á ‡§Æ‡§® ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§¶‡•á‡§∂ ‡§ö‡•Å‡§®‡§æ‡§µ ‡§ï‡§Æ ‡§ï‡§Æ ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞‡•ã ‡§ì‡§°‡§ø‡§∂‡§æ ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§∏‡§æ‡§Æ‡§®‡•á ‡§Æ‡§§‡§¶‡§æ‡§§‡§æ‡§ì‡§Ç ‡§≤‡•ã‡§ï‡§∏‡§≠‡§æ ‡§Æ‡§§‡§¶‡§æ‡§® ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ ‡§¶‡•Ç‡§∏‡§∞‡§æ ‡§Æ‡§§‡§¶‡§æ‡§® ‡§π‡§Æ‡§æ‡§∞‡•á ‡§Æ‡§§‡§¶‡§æ‡§§‡§æ ‡§∏‡§Æ‡§Ø ‡§µ‡§ø‡§µ‡•á‡§ï ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ö‡•Å‡§® ‡§™‡•Ä‡§è‡§Æ', '‡§ú‡§º‡•Å‡§≤‡•ç‡§Æ ‡§á‡§§‡§®‡§æ ‡§¨‡•Å‡§∞‡§æ ‡§ú‡§ø‡§§‡§®‡•Ä ‡§¨‡•Å‡§∞‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡•Ä ‡§ñ‡§æ‡§Æ‡•ã‡§∂‡•Ä ‡§π‡§ï‡§º ‡§¨‡•ã‡§≤‡§®‡§æ ‡§∏‡•Ä‡§ñ‡•ã ‡§µ‡§∞‡§®‡§æ ‡§™‡•Ä‡§¢‡§ø‡§Ø‡§æ‡§Ç ‡§ó‡•Ç‡§Ç‡§ó‡•Ä ‡§ú‡§æ‡§è‡§Ç‡§ó‡•Ä', '‡§∏‡§µ‡§ø‡§ß‡§æ‡§® ‡§∞‡•á‡§™ ‡§ú‡§æ‡§Ø‡§ú', '‡§Ü‡§§‡§æ ‡§Æ‡§æ‡§ú‡•Ä ‡§∏‡§§‡§ï‡§≤‡•Ä ‡§≠‡•á‡§®‡§ö‡•ã‡§¶ ‡§≠‡•ã‡§∏‡•ç‡§°‡§ø‡§ï‡•á ‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶ ‡§π‡§ø‡§ú‡§°‡§º‡•á ‡§∏‡•Å‡§® ‡§§‡•Å‡§ú‡•á ‡§á‡§Æ‡§∞‡§æ‡§® ‡§ñ‡§æ‡§® ‡§â‡§∏‡§ï‡§æ ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§® ‡§á‡§§‡§®‡§æ ‡§Ö‡§ö‡•ç‡§ö‡§≤ ‡§≤‡§ó‡§§‡§æ ‡§§‡•Ç ‡§Æ‡§æ‡§Å ‡§õ‡•Å‡§°‡§º‡§µ‡§æ‡§®‡•á ‡§µ‡§π‡§æ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ö‡§≤‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡§Æ‡§æ‡§∞‡•Ä ‡§Æ‡§æ‡§Å ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ö‡•ã‡§¶‡§§', '‡§Ü‡§ú‡§Æ ‡§ñ‡§æ‡§® ‡§¨‡•ã‡§≤‡§§‡§æ ‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶ ‡§â‡§°‡§º‡§æ ‡§∏‡§æ‡§≤‡•á']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je09nozLmmMm"
      },
      "source": [
        "## 1.3 Build the vocabulary (0.5 + 0.5 points)\r\n",
        "\r\n",
        "The input to the first layer of word2vec is an one-hot encoding of the current word. The output od the model is then compared to a numeric class label of the words within the size of the skip-gram window. Now\r\n",
        "\r\n",
        "* Compile a list of all words in the development section of your corpus and save it in a variable ```V```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpoGmTKx-AOQ"
      },
      "source": [
        "#TODO: implement!\n",
        "V = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiaVglVNoENY"
      },
      "source": [
        "* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqPNw6IT-AOQ"
      },
      "source": [
        "#TODO: implement!\n",
        "def word_to_one_hot(word):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKD8zBlxVclh"
      },
      "source": [
        "## 1.4 Subsampling (0.5 points)\r\n",
        "\r\n",
        "The probability to keep a word in a context is given by:\r\n",
        "\r\n",
        "$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.001}}+1\\Big) \\cdot \\frac{0.001}{z(w_i)}$\r\n",
        "\r\n",
        "Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\r\n",
        "* Calculate word frequencies\r\n",
        "* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj4sDOVMMr0b"
      },
      "source": [
        "#TODO: implement!\r\n",
        "def sampling_prob(word):\r\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxV1P90zplxu"
      },
      "source": [
        "# 1.5 Skip-Grams (1 point)\r\n",
        "\r\n",
        "Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \r\n",
        "\r\n",
        "* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\r\n",
        "\r\n",
        "* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8CCTpVy-AOR"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "def get_target_context(sentence):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfEFgtkmuDjL"
      },
      "source": [
        "# 1.6 Hyperparameters (0.5 points)\r\n",
        "\r\n",
        "According to the word2vec paper, what would be a good choice for the following hyperparameters? \r\n",
        "\r\n",
        "* Embedding dimension\r\n",
        "* Window size\r\n",
        "\r\n",
        "Initialize them in a dictionary or as independent variables in the code block below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xSKuFJcYoD"
      },
      "source": [
        "# Set hyperparameters\n",
        "window_size = \n",
        "embedding_size = \n",
        "\n",
        "# More hyperparameters\n",
        "learning_rate = 0.05\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiM2zq-YunPx"
      },
      "source": [
        "# 1.7 Pytorch Module (0.5 + 0.5 + 0.5 points)\r\n",
        "\r\n",
        "Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\r\n",
        "\r\n",
        "* Initialize the two weight matrices of word2vec as fields of the class.\r\n",
        "\r\n",
        "* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\r\n",
        "\r\n",
        "* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9sGNytYhwxS",
        "outputId": "41645b64-e4ed-4e6a-e10f-74cb39b92230"
      },
      "source": [
        "# Create model \n",
        "\n",
        "class Word2Vec(Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(\n",
            "  (input): Linear(in_features=534, out_features=300, bias=False)\n",
            "  (output): Linear(in_features=300, out_features=534, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XefIDMMHv5zJ"
      },
      "source": [
        "# 1.8 Loss function and optimizer (0.5 points)\r\n",
        "\r\n",
        "Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9-Ino-e29w3"
      },
      "source": [
        "# Define optimizer and loss\n",
        "optimizer = \n",
        "criterion = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckTfK78Ew8wI"
      },
      "source": [
        "# 1.9 Training the model (3 points)\r\n",
        "\r\n",
        "As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\r\n",
        "\r\n",
        "* Load the weights saved in 1.6 at the start of every execution of the code block\r\n",
        "* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\r\n",
        "* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\r\n",
        "\r\n",
        "You can play around with the number of epochs and the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbMGD5L0mLDx"
      },
      "source": [
        "# Define train procedure\n",
        "\n",
        "# load initial weights\n",
        "\n",
        "def train():\n",
        " \n",
        "  print(\"Training started\")\n",
        "\n",
        "train()\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgQkaYstyj0Q"
      },
      "source": [
        "# 1.10 Train on the full dataset (0.5 points)\r\n",
        "\r\n",
        "Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \r\n",
        "\r\n",
        "* Then, retrain your model on the complete dataset.\r\n",
        "\r\n",
        "* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x8hQP_bg4_g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}